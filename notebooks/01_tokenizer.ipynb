{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9ffb2f",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5865afa",
   "metadata": {},
   "source": [
    "Our goal here is to transform our text into tokens.\n",
    "\n",
    "Each token will have a unique ID that will correspond to an index in the embedding matrix.\n",
    "\n",
    "The mapping text -> token -> ID -> embedding vector will translate text into vector of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa64f388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scène I\n",
      "Le Barbouillé\n",
      "Il faut avouer que je suis le plus malheureux de tous les hommes. J'ai une femme qui me fait enrager :  au lieu\n",
      "de me donner du soulagement et de faire les choses à mon souhait, elle me fait donner au diable vingt fois le\n",
      "jour ;  au lieu de se tenir à la maison, elle aime la promenade, la bonne chère, et fréquente je ne sais quelle\n",
      "sorte de gens. Ah !  pauvre Barbouillé, que tu es misérable !  Il faut pourtant la punir. Si je la tuois...\n",
      "L'invention ne vaut rien, car tu serois pendu. Si tu la faisois mettre en prison... La carogne en sortiroit avec\n",
      "son passe−partout. Que diable faire donc ?  Mais voilà Monsieur le Docteur qui passe par ici :  il faut que je\n",
      "lui demande un bon conseil sur ce que je dois faire.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the text file\n",
    "with open(\"../data/tinymoliere.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[208:949])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e080e42",
   "metadata": {},
   "source": [
    "## Naive char level tokenization\n",
    "\n",
    "Probably the most simple way to convert text into token is to work at char level i.e. each token corresponds to a char in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9b25633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 91\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', '°', 'Ç', 'É', 'à', 'â', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'î', 'ï', 'ò', 'ô', 'ù', 'û', 'œ', '−']\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = sorted(set(text)) # we sort the tokens to have consistent such that we can use indexing as ID\n",
    "print(f\"Number of unique tokens: {len(unique_tokens)}\")\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a091b",
   "metadata": {},
   "source": [
    "This dataset contains 91 unique char (including french accents, capitilized letters and punctuations).\n",
    "\n",
    "Now we can use the unique tokens extracted to create the tokenizer (== mapping between token and id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17bf6fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token into id\n",
    "token_to_id = {token: i for i, token in enumerate(sorted(unique_tokens))}\n",
    "\n",
    "# convert id into token\n",
    "id_to_token = {i: token for token, i in token_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e846e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 49, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# few token to id examples\n",
    "token_to_id[\"a\"], token_to_id[\"b\"], token_to_id[\"c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d39bd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'b', 'c')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# few id to token examples\n",
    "id_to_token[48], id_to_token[49], id_to_token[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88b7edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what will be useful later is to have a function that convert text into list of token ids\n",
    "def tokenize(text: str) -> list[int]:\n",
    "    # as we are splitting at char level it's quite straightforward\n",
    "    return [token_to_id[token] for token in text]\n",
    "\n",
    "def untokenize(token_ids: list[int]) -> str:\n",
    "    # we can use the id_to_token mapping to convert back to text\n",
    "    return \"\".join(id_to_token[id] for id in token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed4d3672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 1, 49, 1, 50]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"a b c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45116a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a b c'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untokenize(tokenize(\"a b c\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c52f25a",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb116c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0131af15",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
